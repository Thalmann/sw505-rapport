\section{Lokalisering}
Når robotten skal kortlægge sine omgivelser er det nødvendigt at vide hvor den befinder sig.
Til at finde positionen har vi overvejet flere forskellige metoder til lokalisering, f.eks. via:
\begin{itemize}
\item Microsoft Kinect vs. Nintendo Wiimote
\item Dead reckoning (lokalisering uden ekstern 'hjælp')
\item Billedgenkendelse
\end{itemize}

\subsection{Dead reckoning}
Dead Reckoning er en teknik til at finde sin position efter ens bevægelser uden eksterne sensorer.
Robotten ved hvor den starter, og beregner sin nye position efter hvilken kommando den har udført.\cite{deadrec}

Denne teknik er ikke det bedste valg i de situationer hvor man ønsker en meget præcis lokalisering af robotten, hvilket skyldes de usikkerheder for motorer og sensorer som man ofte arbejder med.

\subsection{Wiimote}
En anden sensor der kan bruges er Nintendos Wiimote. 
Wiimote en optisk sensor til at lokalisere infrarødt lys fra den tilhørende sensor-bar.
Ved hjælp af triangulering af det modtagne lys kan Wiimoten bestemme sin position. \cite{wiimote}

\subsection{Kinect }
Som nævnt i \cref{kinect:komponenter}, benytter Microsofts Kinect primært et RGB farvekamera samt en dybdesensor til at fortolke bevægelse som input i applikationer.
Kinecten giver mange muligheder i forhold til dens anvendelse, da den indeholder komponenter som kan benyttes uafhængigt af hinanden. 
Da det er forholdsvist let at tilgå komponenterne via det medfølgende SDK, og at det er rigtig godt dokumenteret samt mængden af sensorer tilgængelige, vil Kinecten være et godt valg, da den giver mulighed for at afteste flere typer af løsninger på lokaliseringsproblemet.


\section{Valg af lokaliseringsmetode}
\thilemann{vær venligst kritisk overfor nedenstående tekst i dette afsnit når der laves review...}
Der blev foretaget en indledende test af de foreslåede lokaliseringsmetoder for at vælge hvilken der passede bedst til dette projekt.

Dead reckoning muligheden blev hurtigt udelukket, da gruppen ville fokusere på kortlægningsopgaven.
Dead reckoning ville også kræve en del mere arbejde ift. implementering end de andre lokaliseringsmetoder, da den vil introducere mange usikkerheder som ikke eksisterer i de andre metoder (da de andre flytter usikkerhederne fra robotten til eksterne sensorer).

Efter sammenligning af Kinect og Wiimote blev det besluttet at fokusere på Kinect; primært pga. de flere muligheder den giver med hensyn til implementering, men også det faktum at den var tilgængelig via universitetet.

Til at begynde med blev alle Kinectens muligheder undersøgt, og strategien på daværende tidspunkt var at benytte dens farvekamera sammen med de infrarøde sensorer (til dybdemålinger). 
Dette introducerede imidlertid en del usikkerheder til hvor og hvordan Kinecten skulle placeres i forhold til kørselsmiljøet for at oversætte Kinectens koordinater til reelle koordinater gældende i selve kørselsmiljøet.
En simplificering af opsætningen, med Kinecten monteret i en loftsplade med syn vinkelret ned på kørselsmiljøet, gjorde imidlertid Kinectens mange sensorer unødvendige da det via simple beregner således ville gøre det muligt at finde de korrekte koordinater.
Beregningen af de reelle koordinater er beskrevet i \cref{lokalisering:punktomregning}.

Ovenstående simplificering gør det muligt at fokusere mere på kortlægning af robottens kørselsmiljø og dermed fjerne fokus fra at lokalisere robotten.
Ved test er det derfor valgt at fokusere på lokalisere robotten via identificering af farver på robotten frem for f.eks. at benytte OpenCV til \textit{blob-genkendelse}\footnote{Et \textit{blob} betegner området af et billede som genkendes ud fra dets karakteristikker.} eller \textit{background substraction}\footnote{Ved background substraction trækkes et statisk billede af kørselsmiljøet fra selve videoen, for at fremhæve dets forskelle.}.
De to omtalte metoder har været i brug, men da resultaterne fra farvesporing var overlegne, vil det fremover være metoden der fokuseres på.


%\clearpage
\section{Omregning fra punkt i billede til reelt punkt}\label{lokalisering:punktomregning}
Det billede der fås ind fra kameraet, består af en mængde pixels.
Robottens centrum, og derved lokation, vil være i præcist én pixel.
For at robotten skal kunne bruge denne til at navigere i den virkelige verden, er det nødvendigt at omregne pixel(-koordinat) til reelt koordinat.

\subsection{Ensvinklede trekanter}
Den overordnede idé er at bruge reglen om ensvinklede trekanter, da det passer på situationen som kan ses i \cref{fig:kameratrekant}.
Her er $v$ den synsvinkel som kameraet har i den givne dimension; for Kinect'en er det $57^\circ$ i bredden og $43^\circ$ i højden.
$v$ er blot med for at vise at der er tale om ensvinklede trekanter.
$x_1$ er størrelse af den givne dimension for det billede der dannes af Kinect'en.
$x_2$ er tilsvarende $x_1$, blot den reelle størrelse af det billede der gives.

\begin{figure}[h]
\centering
\begin{tikzpicture}[extline/.style={shorten >=-#1,shorten <=-#1},
  extline/.default=1cm]

\coordinate (A) at (0,0);
\coordinate (B) at (2,6);
\coordinate (C) at (4,0);
\coordinate (A2) at (1,3);
\coordinate (C2) at (3,3);

\draw [dashed] (A) -- (B);
\draw [dashed] (B) -- (C);
\draw (A) -- (C) node [midway,below] {$x_2$};
\draw (A2) -- (C2) node [midway,below] {$x_1$};
\draw ($(B) + (-110:0.55)$) arc (-110:-70:0.55) node [right] {$v$};
\end{tikzpicture}
\caption{Situationen med de kendte variable}
\label{fig:kameratrekant}
\end{figure}

\begin{description}
\item[Sætning:]{Hvis alle vinkler i to trekanter er parvis lige store, er siderne parvis proportionale.}
\end{description}
Som det kan ses i \cref{fig:udregningstrekant} er der tale om netop sådan en trekant for vores situation.
Trekanten er halveret for at midten får koordinatet $(0,0)$, derved går en dimension fra $-\frac{1}{2}x$ til $+\frac{1}{2}x$.

\subsection{Beregning}
For at finde proportionalitetsfaktoren deles en vilkårlig side af den store trekant med den tilsvarende side fra den lille trekant.
Her vil proportionalitetsfaktoren derved blive $f = \frac{\frac{1}{2}x_2}{\frac{1}{2}x_1}$.

For at omregne en pixel $p$ om til et reelt koordinat, skal der blot påregnes proportionalitetsfaktoren.
Så derved får vi vores reelle koordinat $k = f \cdot p$.
For vores sitation er der 2 dimensioner; længde og bredde: \begin{equation*}
(k_1,k_2) = (f \cdot p_1, f \cdot p_2)
\end{equation*}


\begin{figure}[h]
\centering
\begin{tikzpicture}[extline/.style={shorten >=-#1,shorten <=-#1},
  extline/.default=1cm]

\coordinate (A) at (0,0);
\coordinate (B) at (0,6);
\coordinate (C) at (2,0);
\coordinate (A2) at (0,3);
\coordinate (C2) at (1,3);
\coordinate (mAC) at (1.25,0);
\coordinate (oldA) at (-2,0);

\draw [dashed,color=gray] (oldA) -- (B);
\draw [dashed,color=gray] (oldA) -- (A);
\draw [thick,dotted] (mAC) -- (B);
\draw (A) -- (B);
\draw (B) -- (C);
\draw (A) -- (C) node [below,midway,xshift=-1cm] {$0$} node [below,midway,xshift=1.2cm] {$\frac{1}{2}x_2$};
\draw (A2) -- (C2) node [midway,xshift=1cm] {$\frac{1}{2}x_1$} node [midway,xshift=-0.7cm] {$0$};
\draw ($(B) + (-90:0.55)$) arc (-90:-70:0.55) node [right] {$\frac{1}{2}v$};
\filldraw (0.63,3) circle (0.05cm) node [left,yshift=-0.3cm] {$p$};
\filldraw (1.25,0) circle (0.05cm) node [left,yshift=-0.3cm] {$k$};
\end{tikzpicture}
\caption{Selve omregningen}
\label{fig:udregningstrekant}
\end{figure}

\section{Opsummering}
Dette afsnit fokuserer på de forskellige metoder der har været overvejet i forhold til at lokalisere robotten i dens kørselsmiljø.
Microsoft Kinect blev valgt pga. dens mangfoldighed af sensorer og dens brugervenlighed ift. implementering via officielt understøttet SDK fra Microsoft.

Ved placering af Kinect i en loftsplade blev lokaliseringsproblemet simplificeret væsentligt, og gjorde det derved muligt at spore robotten via dens farveforskelle.

Slutteligt beskrives teorien bag omregningen fra pixel koordinater i Kinectens farvekamera til reelle koordinater, således robotten kan spørge på sin position og få svar tilbage fra computeren med reelle koordinater den kan reagere på.