\section{Bayes Theorem}


\subsection{Semantics of probability}

% Muligvis unødvendig.
\subsubsection{Sandsynlighed som mål over mulige verdener}

Et sandsynligheds mål $\mu$ er en funktion fra en mængde verdener til mængden af positive reelle tal. 
Hvis det gælder at $\Omega_1 \cap \Omega_2 = {}$ hvor $\Omega_1$ og $\Omega_2$ er mænger af verdener.
Har vi at 
\begin{enumerate}
\item $\mu(\Omega_1 \cup \Omega_2) = \mu(\Omega_1) + \mu(\Omega_2)$
\item Hvis $\Omega$ er mængden af alle verdener vil $\mu(\Omega) = 1$ 
\end{enumerate}

Sandsynlighedsmålet kan udviddes til at dække sandsynligheden for enkelte verdener således.
$$\mu(\omega) = \mu(\{\omega\})$$

Notationen kan udviddes til at dække propositioner hvor $P(\alpha)$ er målet af mængden af verdener hvor
propositionen $\alpha$ er sand.
dvs.
$$P(\alpha) = \mu(\{\omega \mid \omega \models \alpha \})$$
Her betyder notationen $\omega \models \alpha$ at propositionen $\alpha$ er sand i verdenen $\omega$.

Notationen kan yderligere udviddes til at dække \emph{Random Variables}.
Hvor en sandsynlighedsfordeling $P(X)$ over variablen X er en funktion fra
domænet for X til mængden af positive reelle tal.
Således at givet $x \in dom(X)$ vil $P(x)$ være sandsynligheden for at $X = x$.

Denne notation kan også benyttes til at beskrive sandsynligheder af flere variabler.
f.eks. er P(X,Y) en sandsynlighedsfordeling over variablerne X og Y således at $P(X = x \wedge Y = y)$ hvor
$x \in dom(X)$ og $y \in dom(Y)$, har værdien $P(X = x \wedge Y = y)$ hvor
$X = x \wedge Y = y$ er en proposition og P er funktionen over propositioner. 

\subsection{Conditional probability}

Målet af 'troen' på en given proposition \emph{h} baseret på propositionen e kaldes
den betingede sandsynlighed for h givet e og skrives $P(h \mid e)$.

Hvis alle en agents opservationer om verden kaldes dens \emph{bevis} og benævnes \emph{e}




\subsubsection{Uafhængighed}



\subsubsection{Bayes Rule}

$$P(h \mid e) = \frac{P(e \mid h) \times P(h)}{P(e)}$$
Hvor $P(h \mid e)$ er den \emph{posterior} sandsynlighed, $P(e \mid h)$ er \emph{likelihood} og $P(h)$ er \emph{prior}. 


\subsection{Belief Networks}







\subsection{Binære Bayes filtre med statisk tilstand}
I tilfælde hvor tilstanden er statisk, kan der anvendes binære Bayes filtre.
Her kan en \textit{belief} på en given tilstand $x$ beskrives som:
\begin{equation}
bel_t(x) = p(x \mid z_{1:t},u_{1:t}) = p(x \mid z_{1:t})
\end{equation}

Hvor tilstanden $x$ er binær, dvs. enten sand ($x$) eller falsk ($\lnot x$).
Derved har vi at $bel_t(\lnot x) = 1 - bel_t(x)$.
Bemærk desuden at kontrol data $u_{1:t}$ er overflødig, da lokationen til enhver tid er kendt.

\subsubsection{Log Odds}
For at overkomme eventuelle afrundings-unøjagtigheder, indføres der en funktion \textit{log odds ratio}, der konverterer sandsynlighedsværdierne fra $[0;1]$ til $[-\infty;\infty]$.

\begin{equation}
l(x) = \log \frac{p(x)}{1 - p(x)}
\end{equation}

For igen at få vores \textit{belief} tilbage ud fra en \textit{log odds}, benyttes følgende ligning:
\begin{equation}
bel_t(x) = 1 - \frac{1}{1 + exp\{l_t\}}
\end{equation}

\subsubsection{Opdateringsalgoritmen}
Opdateringsalgoritmen tager \textit{log odds} for en \textit{posterior belief} samt en ny sensor-måling, hvor ud fra der returneres en \textit{log odds} for en ny \textit{belief}.
Algoritmen, skrevet i pseudo-kode, kan ses i \cref{alg:binaerbayesfilter}.

\begin{algorithm}[h]
\textbf{BinaryBayesFilter($l_{t-1}, z_t$)} \\
\Indp $l_t = l_{t-1} + \log \frac{p(x \mid z_t)}{1-p(x \mid z_t)} - \log \frac{p(x)}{1-p(x)}$ \\
\Return{$l_t$}
\caption{Binært Bayes filter algoritme}
\label{alg:binaerbayesfilter}
\end{algorithm}
